{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        pass\n",
    "    \n",
    "    def backward(self, input, grad_output):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = torch.max(input, torch.tensor([0.], device='cpu'))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, input_units, output_units, learning_rate=0.1):\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # initialize weights with small random numbers. We use normal initialization, \n",
    "        # but surely there is something better. Try this once you got it working: http://bit.ly/2vTlmaJ\n",
    "        self.weights = torch.randn(input_units, output_units, device='cpu', requires_grad=True)*0.01\n",
    "        self.biases  = torch.ones(output_units, device='cpu', requires_grad=True)\n",
    "        \n",
    "    def forward(self,input):\n",
    "        return torch.matmul(input, self.weights) + self.biases\n",
    "    \n",
    "    def backward(self, w_grad, b_grad):\n",
    "        self.weights = self.weights - self.learning_rate * w_grad\n",
    "        self.biases  = self.biases - self.learning_rate * b_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f715bf1c4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from mnist import load_dataset\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset(flatten=True)\n",
    "\n",
    "plt.figure(figsize=[6,6])\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.title(\"Label: %i\"%y_train[i])\n",
    "    plt.imshow(X_train[i].reshape([28,28]),cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = torch.from_numpy(X_train).float().to('cpu')\n",
    "X_train.requires_grad_(True)\n",
    "\n",
    "X_val = torch.from_numpy(X_val).float().to('cpu')\n",
    "X_val.requires_grad_(True)\n",
    "\n",
    "X_test = torch.from_numpy(X_test).float().to('cpu')\n",
    "X_test.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(network, X):\n",
    "    activations = []\n",
    "    input       = X\n",
    "    \n",
    "    for layer in network:\n",
    "        input = layer.forward(input)\n",
    "        activations.append(input)\n",
    "        \n",
    "    assert len(activations) == len(network)\n",
    "    return activations\n",
    "\n",
    "def predict(network, X):\n",
    "    logits = forward(network, X)[-1]\n",
    "    return logits.cpu().detach().numpy().argmax(axis=-1)\n",
    "\n",
    "def normalize(vec, c):\n",
    "    norm = torch.norm(vec)\n",
    "    \n",
    "    if norm >= c:\n",
    "        return (c * vec) / norm\n",
    "    else:\n",
    "        return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LRA-diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network,X,y):\n",
    "    \n",
    "    # parameters\n",
    "    K        = 5\n",
    "    beta     = .001\n",
    "    n_layers = len(network) // 2 + 1\n",
    "    layer    = n_layers\n",
    "    c        = 1\n",
    "    \n",
    "    # forward pass\n",
    "    act = forward(network, X)\n",
    "    \n",
    "    pre_activations      = act[::2] # h\n",
    "    pre_activations_tmp  = pre_activations.copy() # h_hat\n",
    "\n",
    "    post_activations     = [X] + act[1::2] # z\n",
    "    layer_post_target    = post_activations.copy() # y_z\n",
    "    post_activations_tmp = post_activations.copy() # z_hat\n",
    "\n",
    "    layer_post_target    = layer_post_target + [torch.from_numpy(y).long().to('cpu')]\n",
    "    W_grads              = []\n",
    "    b_grads              = []\n",
    "    \n",
    "    for l in range(len(network) - 1, 0, -2):\n",
    "        W                   = torch.from_numpy(network[l].weights.cpu().detach().numpy()).to('cpu')\n",
    "        W.requires_grad_(True)\n",
    "        \n",
    "        b                   = torch.from_numpy(network[l].biases.cpu().detach().numpy()).to('cpu')\n",
    "        b.requires_grad_(True)\n",
    "        \n",
    "        z_l_1                = torch.from_numpy(post_activations[layer-1].cpu().detach().numpy()).to('cpu') # z_(l-1)\n",
    "        z_l_1.requires_grad_(True)\n",
    "        \n",
    "        z_l                  = torch.matmul(z_l_1, W) + b # z_l\n",
    "        \n",
    "        if l == len(network) - 1: \n",
    "            loss = torch.functional.F.nll_loss(z_l, layer_post_target[layer])\n",
    "        else:\n",
    "            mse_loss = torch.nn.MSELoss()\n",
    "            loss     = mse_loss(z_l, layer_post_target[layer])\n",
    "            \n",
    "        loss.backward(retain_graph=True)\n",
    "        \n",
    "        w_grad = normalize(W.grad, c)\n",
    "        b_grad = normalize(b.grad, c)\n",
    "\n",
    "        W_grads.append(w_grad)\n",
    "        b_grads.append(b_grad)\n",
    "        \n",
    "        # zero out gradients\n",
    "        W.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "        \n",
    "        for k in range(K):\n",
    "            h_l_1  = torch.from_numpy(pre_activations[layer-2].cpu().detach().numpy()).to('cpu')\n",
    "            h_l_1.requires_grad_(True)\n",
    "\n",
    "            z_l_1  = network[l-1].forward(h_l_1)\n",
    "            z_l    = torch.matmul(z_l_1, W) + b\n",
    "            \n",
    "            if l == len(network) - 1:\n",
    "                loss   = torch.functional.F.nll_loss(z_l, layer_post_target[layer])\n",
    "            else:\n",
    "                loss = torch.nn.MSELoss()\n",
    "                loss = mse_loss(z_l, layer_post_target[layer])\n",
    "            \n",
    "            loss.backward(retain_graph=True)    \n",
    "            h_l_1_grad            = h_l_1.grad\n",
    "            \n",
    "            # normalize h_l_1_grad\n",
    "            h_l_1_grad = normalize(h_l_1_grad, c)\n",
    "            \n",
    "            # updates\n",
    "            pre_activations[layer-2]  = h_l_1 - beta * h_l_1_grad\n",
    "            post_activations[layer-1] = network[l-1].forward(pre_activations[layer-2])\n",
    "            pre_activations[layer-1]  = torch.matmul(z_l_1, W) + b\n",
    "            \n",
    "            h_l_1.grad.zero_() # zero out gradients\n",
    "\n",
    "#             if layer != n_layers-1:\n",
    "#                 post_activations[layer-1] = network[l].forward(pre_activations[layer-1])\n",
    "        \n",
    "        layer_post_target[layer-1] = post_activations[layer-1]\n",
    "        layer -= 1\n",
    "    \n",
    "        \n",
    "    return W_grads[::-1], b_grads[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(len(inputs))\n",
    "    for start_idx in trange(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "train_log = []\n",
    "val_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = []\n",
    "network.append(Dense(X_train.shape[1], 100))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(100, 200))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(200, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 14/1562 [00:00<00:11, 135.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "tensor([[ 0.0010,  0.0153,  0.0019, -0.0027, -0.0112, -0.0149, -0.0089, -0.0063,\n",
      "          0.0070, -0.0091],\n",
      "        [ 0.0029,  0.0206,  0.0009, -0.0133, -0.0054, -0.0060, -0.0025,  0.0008,\n",
      "          0.0107, -0.0115],\n",
      "        [-0.0073,  0.0179, -0.0159,  0.0170,  0.0047,  0.0011,  0.0069,  0.0126,\n",
      "          0.0193,  0.0104],\n",
      "        [-0.0046, -0.0135,  0.0012,  0.0107,  0.0106,  0.0015, -0.0012, -0.0063,\n",
      "         -0.0076,  0.0025],\n",
      "        [ 0.0078, -0.0043,  0.0216, -0.0168,  0.0042,  0.0143, -0.0182, -0.0004,\n",
      "          0.0041,  0.0085],\n",
      "        [ 0.0083, -0.0045, -0.0156,  0.0046,  0.0101,  0.0081, -0.0102, -0.0042,\n",
      "          0.0006,  0.0060],\n",
      "        [-0.0148,  0.0040,  0.0096,  0.0136, -0.0018,  0.0040,  0.0085, -0.0266,\n",
      "          0.0046, -0.0090],\n",
      "        [ 0.0062,  0.0019, -0.0083,  0.0001, -0.0128,  0.0093, -0.0004,  0.0035,\n",
      "         -0.0026,  0.0045],\n",
      "        [-0.0056,  0.0148, -0.0023,  0.0096,  0.0073,  0.0004,  0.0037, -0.0071,\n",
      "         -0.0017, -0.0171],\n",
      "        [-0.0027,  0.0010, -0.0141,  0.0032,  0.0067, -0.0008, -0.0226,  0.0041,\n",
      "          0.0178,  0.0015]], grad_fn=<SliceBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1562/1562 [00:12<00:00, 121.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tensor([[3.1324, 3.6260, 3.1680, 3.2393, 3.0771, 2.8488, 3.1505, 3.2793, 3.0825,\n",
      "         3.1676],\n",
      "        [2.8947, 3.3530, 2.9242, 2.9807, 2.8439, 2.6380, 2.9106, 3.0315, 2.8515,\n",
      "         2.9193],\n",
      "        [3.5321, 4.1009, 3.5622, 3.6784, 3.4931, 3.2394, 3.5757, 3.7254, 3.4987,\n",
      "         3.6016],\n",
      "        [3.1344, 3.6100, 3.1779, 3.2623, 3.1076, 2.8734, 3.1684, 3.2892, 3.0775,\n",
      "         3.1880],\n",
      "        [3.5959, 4.1307, 3.6484, 3.6952, 3.5376, 3.2960, 3.5987, 3.7613, 3.5290,\n",
      "         3.6469],\n",
      "        [2.8391, 3.2605, 2.8467, 2.9360, 2.8008, 2.5984, 2.8449, 2.9667, 2.7829,\n",
      "         2.8783],\n",
      "        [3.2315, 3.7496, 3.2936, 3.3750, 3.1987, 2.9742, 3.2843, 3.3791, 3.1951,\n",
      "         3.2842],\n",
      "        [2.7459, 3.1594, 2.7600, 2.8336, 2.6867, 2.5144, 2.7602, 2.8757, 2.6893,\n",
      "         2.7828],\n",
      "        [2.6524, 3.0775, 2.6853, 2.7598, 2.6260, 2.4318, 2.6815, 2.7785, 2.6110,\n",
      "         2.6786],\n",
      "        [2.9420, 3.3970, 2.9650, 3.0533, 2.9089, 2.6922, 2.9472, 3.0932, 2.9119,\n",
      "         2.9878]], grad_fn=<SliceBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/1562 [00:00<00:14, 106.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "tensor([[3.1324, 3.6260, 3.1680, 3.2393, 3.0771, 2.8488, 3.1505, 3.2793, 3.0825,\n",
      "         3.1676],\n",
      "        [2.8947, 3.3530, 2.9242, 2.9807, 2.8439, 2.6380, 2.9106, 3.0315, 2.8515,\n",
      "         2.9193],\n",
      "        [3.5321, 4.1009, 3.5622, 3.6784, 3.4931, 3.2394, 3.5757, 3.7254, 3.4987,\n",
      "         3.6016],\n",
      "        [3.1344, 3.6100, 3.1779, 3.2623, 3.1076, 2.8734, 3.1684, 3.2892, 3.0775,\n",
      "         3.1880],\n",
      "        [3.5959, 4.1307, 3.6484, 3.6952, 3.5376, 3.2960, 3.5987, 3.7613, 3.5290,\n",
      "         3.6469],\n",
      "        [2.8391, 3.2605, 2.8467, 2.9360, 2.8008, 2.5984, 2.8449, 2.9667, 2.7829,\n",
      "         2.8783],\n",
      "        [3.2315, 3.7496, 3.2936, 3.3750, 3.1987, 2.9742, 3.2843, 3.3791, 3.1951,\n",
      "         3.2842],\n",
      "        [2.7459, 3.1594, 2.7600, 2.8336, 2.6867, 2.5144, 2.7602, 2.8757, 2.6893,\n",
      "         2.7828],\n",
      "        [2.6524, 3.0775, 2.6853, 2.7598, 2.6260, 2.4318, 2.6815, 2.7785, 2.6110,\n",
      "         2.6786],\n",
      "        [2.9420, 3.3970, 2.9650, 3.0533, 2.9089, 2.6922, 2.9472, 3.0932, 2.9119,\n",
      "         2.9878]], grad_fn=<SliceBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1562/1562 [00:15<00:00, 103.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tensor([[6.0556, 7.0011, 6.1324, 6.2698, 5.9653, 5.5276, 6.1019, 6.3546, 5.9672,\n",
      "         6.1323],\n",
      "        [5.7747, 6.6776, 5.8447, 5.9665, 5.6887, 5.2768, 5.8179, 6.0602, 5.6936,\n",
      "         5.8389],\n",
      "        [7.1879, 8.3210, 7.2695, 7.4683, 7.1051, 6.5889, 7.2678, 7.5715, 7.1072,\n",
      "         7.3092],\n",
      "        [6.0624, 6.9919, 6.1483, 6.2985, 6.0010, 5.5570, 6.1256, 6.3702, 5.9675,\n",
      "         6.1580],\n",
      "        [7.2963, 8.4005, 7.3998, 7.5306, 7.1923, 6.6857, 7.3348, 7.6531, 7.1803,\n",
      "         7.3984],\n",
      "        [5.6533, 6.5084, 5.7000, 5.8534, 5.5809, 5.1770, 5.6863, 5.9272, 5.5601,\n",
      "         5.7320],\n",
      "        [6.3464, 7.3473, 6.4538, 6.6050, 6.2766, 5.8294, 6.4302, 6.6569, 6.2699,\n",
      "         6.4445],\n",
      "        [5.5606, 6.4069, 5.6132, 5.7509, 5.4670, 5.0928, 5.6017, 5.8357, 5.4667,\n",
      "         5.6364],\n",
      "        [5.4651, 6.3231, 5.5374, 5.6756, 5.4045, 5.0089, 5.5215, 5.7364, 5.3869,\n",
      "         5.5304],\n",
      "        [5.8541, 6.7602, 5.9188, 6.0731, 5.7858, 5.3607, 5.8880, 6.1570, 5.7867,\n",
      "         5.9409]], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    print('*' * 100)\n",
    "    print(network[-1].weights[:10])\n",
    "        \n",
    "    for x_batch,y_batch in iterate_minibatches(X_train, y_train, batchsize=32, shuffle=True):\n",
    "        \n",
    "        W_grads, b_grads = train(network, x_batch, y_batch)\n",
    "        layer    = 0\n",
    "        \n",
    "        for l in range(2, len(network), 2):\n",
    "            network[l].backward(W_grads[layer], b_grads[layer])\n",
    "            layer += 1\n",
    "    \n",
    "    print()\n",
    "    print(network[-1].weights[:10])\n",
    "        \n",
    "    train_log.append(np.mean(predict(network, X_train) == y_train))\n",
    "    val_log.append(np.mean(predict(network, X_val) == y_val))\n",
    "    \n",
    "#     clear_output()\n",
    "#     print(\"Epoch\",epoch)\n",
    "#     print(\"Train accuracy:\",train_log[-1])\n",
    "#     print(\"Val accuracy:\",val_log[-1])\n",
    "#     plt.plot(train_log,label='train accuracy')\n",
    "#     plt.plot(val_log,label='val accuracy')\n",
    "#     plt.legend(loc='best')\n",
    "#     plt.grid()\n",
    "#     plt.show()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "264px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
